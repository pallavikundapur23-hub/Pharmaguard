"""
Comprehensive System Verification & Diagnostics
Checks all connections, APIs, databases, and core functionality
"""
import sys
import os
from pathlib import Path
import json
import traceback

print("\n" + "="*90)
print("üîç COMPREHENSIVE SYSTEM VERIFICATION & DIAGNOSTICS")
print("="*90)

# Track results
results = {
    "passed": [],
    "failed": [],
    "warnings": []
}

def check(name, test_func):
    """Run a check and track results"""
    try:
        test_func()
        results["passed"].append(name)
        print(f"  ‚úÖ {name}")
        return True
    except Exception as e:
        results["failed"].append((name, str(e)))
        print(f"  ‚ùå {name}")
        print(f"     Error: {str(e)[:100]}")
        return False

def warn(name, message):
    """Add a warning"""
    results["warnings"].append((name, message))
    print(f"  ‚ö†Ô∏è  {name}: {message}")

# ============================================================================
# 1. ENVIRONMENT & CONFIGURATION
# ============================================================================
print("\n[1/10] ENVIRONMENT & CONFIGURATION")

def check_env_file():
    """Check .env file exists and has required keys"""
    env_path = Path(".env")
    assert env_path.exists(), "`.env` file not found"
    
    with open(env_path, 'r') as f:
        content = f.read()
    
    required_keys = ["GROQ_API_KEY"]
    for key in required_keys:
        if key not in content:
            warn("Environment Variables", f"Missing {key} in .env")

check("Environment file (.env) exists", check_env_file)

def check_env_vars():
    """Check environment variables are loaded"""
    from dotenv import load_dotenv
    load_dotenv()
    
    groq_key = os.getenv("GROQ_API_KEY")
    assert groq_key, "GROQ_API_KEY not found in environment"
    assert len(groq_key) > 10, "GROQ_API_KEY appears invalid (too short)"

check("Environment variables loaded", check_env_vars)

# ============================================================================
# 2. PYTHON MODULES & IMPORTS
# ============================================================================
print("\n[2/10] PYTHON MODULES & IMPORTS")

def check_fastapi():
    """Check FastAPI installation"""
    import fastapi
    import uvicorn
    print(f"       FastAPI: {fastapi.__version__}")
    print(f"       Uvicorn: {uvicorn.__version__}")

check("FastAPI & Uvicorn installed", check_fastapi)

def check_pydantic():
    """Check Pydantic installation"""
    import pydantic
    print(f"       Pydantic: {pydantic.__version__}")

check("Pydantic installed", check_pydantic)

def check_core_imports():
    """Check core pharmacogenomics modules"""
    from src.risk_predictor import RiskPredictor
    from src.gene_models import Phenotype, RiskLevel
    from src.genotype_phenotype import GenotypePhenotypeConverter

check("Core modules import", check_core_imports)

def check_llm_imports():
    """Check LLM modules"""
    from src.llm_cache import ExplanationCache
    from src.llm_config import LLMConfig
    from src.llm_prompt_templates import PromptTemplate
    from src.llm_integration import get_explainer
    from backend.src.llm_explainer import LLMExplainer

check("LLM modules import", check_llm_imports)

def check_api_imports():
    """Check API modules"""
    from src.api_models import (
        UploadResponse, AnalyzeRequest, AnalyzeResponse, ResultsResponse
    )
    from src.api_job_manager import get_job_manager
    import api

check("API modules import", check_api_imports)

# ============================================================================
# 3. GROQ API CONNECTION
# ============================================================================
print("\n[3/10] GROQ API CONNECTION")

def check_groq_key():
    """Verify Groq API key exists"""
    from dotenv import load_dotenv
    load_dotenv()
    
    groq_key = os.getenv("GROQ_API_KEY")
    assert groq_key, "GROQ_API_KEY not set"
    print(f"       Key found: {groq_key[:20]}...{groq_key[-10:]}")

check("Groq API key configured", check_groq_key)

def check_groq_connection():
    """Test connection to Groq API"""
    from groq import Groq
    from dotenv import load_dotenv
    
    load_dotenv()
    client = Groq(api_key=os.getenv("GROQ_API_KEY"))
    
    # Test with simple completion
    message = client.chat.completions.create(
        messages=[{"role": "user", "content": "test"}],
        model="llama-3.3-70b-versatile",
        max_tokens=10
    )
    
    assert message.choices[0].message.content, "No response from Groq"
    print(f"       Model: llama-3.3-70b-versatile")
    print(f"       Response: OK")

check("Groq API connection test", check_groq_connection)

def check_llm_explainer():
    """Check LLM explainer is working"""
    from src.risk_predictor import RiskPredictor
    
    predictor = RiskPredictor()
    assert predictor.llm_available, "LLM not available"
    assert predictor.llm_explainer, "LLM explainer not initialized"
    
    provider = predictor.llm_explainer.provider
    model = predictor.llm_explainer.model
    
    print(f"       Provider: {provider}")
    print(f"       Model: {model}")
    
    assert provider == "groq", f"Expected groq provider, got {provider}"
    assert "llama" in model.lower(), f"Expected llama model, got {model}"

check("LLM explainer initialized", check_llm_explainer)

# ============================================================================
# 4. DATABASE & CACHING
# ============================================================================
print("\n[4/10] DATABASE & CACHING")

def check_cache_db():
    """Check cache database"""
    from src.llm_cache import ExplanationCache
    
    cache = ExplanationCache()
    
    # Test cache operations
    test_key = "TEST_VARIANT"
    test_value = "Test explanation"
    
    # Try to get from cache (will be None first time)
    result = cache.get_variant_explanation("TEST", "*1/*1", "Normal", 1.0)
    
    print(f"       Database: llm_cache.db")
    print(f"       Status: Ready")

check("Cache database functional", check_cache_db)

def check_job_manager():
    """Check job manager"""
    from src.api_job_manager import get_job_manager
    
    manager = get_job_manager()
    assert manager.uploads_dir.exists(), "Uploads directory missing"
    assert manager.jobs_dir.exists(), "Jobs directory missing"
    assert manager.results_dir.exists(), "Results directory missing"
    
    print(f"       Uploads: {manager.uploads_dir}")
    print(f"       Jobs: {manager.jobs_dir}")
    print(f"       Results: {manager.results_dir}")

check("Job manager ready", check_job_manager)

# ============================================================================
# 5. FILE SYSTEM
# ============================================================================
print("\n[5/10] FILE SYSTEM")

def check_vcf_files():
    """Check sample VCF files"""
    vcf_dir = Path("sample_vcf")
    assert vcf_dir.exists(), "sample_vcf directory not found"
    
    vcf_files = list(vcf_dir.glob("*.vcf"))
    assert len(vcf_files) > 0, "No VCF files found"
    
    for vcf in vcf_files:
        print(f"       - {vcf.name} ({vcf.stat().st_size} bytes)")

check("Sample VCF files available", check_vcf_files)

def check_source_files():
    """Check critical source files"""
    required_files = [
        "api.py",
        "app.py",
        "src/risk_predictor.py",
        "src/gene_models.py",
        "backend/src/llm_explainer.py",
    ]
    
    missing = []
    for file in required_files:
        if not Path(file).exists():
            missing.append(file)
    
    if missing:
        raise FileNotFoundError(f"Missing: {', '.join(missing)}")
    
    print(f"       All {len(required_files)} critical files present")

check("Critical source files exist", check_source_files)

def check_directories():
    """Check important directories"""
    dirs = [
        "src",
        "backend/src",
        "sample_vcf",
        "api_data",
    ]
    
    for dir_name in dirs:
        dir_path = Path(dir_name)
        if not dir_path.exists():
            warn("Directories", f"Missing {dir_name}")
        else:
            print(f"       ‚úì {dir_name}")

check("Directory structure", check_directories)

# ============================================================================
# 6. API ENDPOINTS
# ============================================================================
print("\n[6/10] API ENDPOINTS")

def check_api_endpoints():
    """Check API endpoints are defined"""
    from api import app
    
    endpoints = {}
    for route in app.routes:
        if hasattr(route, 'path'):
            if hasattr(route, 'methods'):
                endpoints[route.path] = list(route.methods)
    
    required = ['/health', '/upload', '/analyze']
    missing = [ep for ep in required if ep not in endpoints]
    
    if missing:
        raise Exception(f"Missing endpoints: {missing}")
    
    for ep, methods in sorted(endpoints.items()):
        if '/results' in ep or any(ep.startswith(r) for r in required + ['/results', '/']):
            print(f"       {ep}: {', '.join(methods)}")

check("API endpoints defined", check_api_endpoints)

def check_api_models():
    """Check API data models"""
    from src.api_models import (
        UploadResponse, AnalyzeRequest, AnalyzeResponse, 
        ResultsResponse, HealthResponse
    )
    
    models = [
        UploadResponse, AnalyzeRequest, AnalyzeResponse,
        ResultsResponse, HealthResponse
    ]
    
    print(f"       {len(models)} data models defined")

check("API data models defined", check_api_models)

# ============================================================================
# 7. VCF PROCESSING
# ============================================================================
print("\n[7/10] VCF PROCESSING")

def check_vcf_parser():
    """Test VCF parsing"""
    from src.risk_predictor import RiskPredictor
    
    predictor = RiskPredictor()
    vcf_file = "sample_vcf/comprehensive_test.vcf"
    
    assert Path(vcf_file).exists(), f"Test VCF not found: {vcf_file}"
    
    result = predictor.predict_from_vcf(vcf_file, ["Codeine"])
    assert result is not None, "VCF processing returned None"
    assert result.get('success'), "VCF processing failed"
    
    print(f"       VCF file: {vcf_file}")
    print(f"       Status: Parsed successfully")

check("VCF file parsing", check_vcf_parser)

def check_risk_prediction():
    """Test risk prediction"""
    from src.risk_predictor import RiskPredictor
    
    predictor = RiskPredictor()
    result = predictor.predict_from_vcf(
        "sample_vcf/warfarin_dose.vcf",
        ["Warfarin"]
    )
    
    assert result['success'], "Prediction failed"
    assert 'json_output' in result, "No JSON output"
    
    json_data = json.loads(result['json_output'])
    assert len(json_data) > 0, "No predictions returned"
    
    print(f"       Predictions: {len(json_data)} drug(s)")
    print(f"       JSON output: Generated")

check("Risk prediction functionality", check_risk_prediction)

# ============================================================================
# 8. LLM EXPLANATIONS
# ============================================================================
print("\n[8/10] LLM EXPLANATIONS")

def check_llm_explanations():
    """Check LLM explanations are generating"""
    from src.risk_predictor import RiskPredictor
    import json
    
    predictor = RiskPredictor()
    result = predictor.predict_from_vcf(
        "sample_vcf/codeine_risk.vcf",
        ["Codeine"]
    )
    
    json_data = json.loads(result['json_output'])
    assessment = json_data[0]
    
    llm = assessment.get('llm_generated_explanation', {})
    metrics = assessment.get('quality_metrics', {})
    
    assert 'variant_interpretation' in llm, "No variant interpretation"
    assert 'risk_explanation' in llm, "No risk explanation"
    assert len(llm['variant_interpretation']) > 20, "Variant interpretation too short"
    assert len(llm['risk_explanation']) > 20, "Risk explanation too short"
    
    print(f"       Variant interpretation: {len(llm['variant_interpretation'])} chars")
    print(f"       Risk explanation: {len(llm['risk_explanation'])} chars")
    print(f"       Dosing guidance: {len(llm.get('dosing_recommendation', ''))} chars")
    print(f"       Monitoring: {len(llm.get('monitoring_guidance', ''))} chars")

check("LLM explanation generation", check_llm_explanations)

def check_quality_metrics():
    """Check quality metrics are tracked"""
    from src.risk_predictor import RiskPredictor
    import json
    
    predictor = RiskPredictor()
    result = predictor.predict_from_vcf(
        "sample_vcf/comprehensive_test.vcf",
        ["Codeine"]
    )
    
    json_data = json.loads(result['json_output'])
    assessment = json_data[0]
    metrics = assessment.get('quality_metrics', {})
    
    required = ['llm_used', 'llm_provider', 'llm_model', 'llm_cached']
    missing = [m for m in required if m not in metrics]
    
    if missing:
        raise Exception(f"Missing metrics: {missing}")
    
    print(f"       LLM Used: {metrics['llm_used']}")
    print(f"       Provider: {metrics['llm_provider']}")
    print(f"       Model: {metrics['llm_model']}")
    print(f"       Cached: {metrics['llm_cached']}")

check("Quality metrics tracking", check_quality_metrics)

# ============================================================================
# 9. CONFIGURATION & SETTINGS
# ============================================================================
print("\n[9/10] CONFIGURATION & SETTINGS")

def check_llm_config():
    """Check LLM configuration"""
    from src.llm_config import LLMConfig
    
    config = LLMConfig.from_env()
    assert config, "LLM config failed to load"
    assert config.temperature > 0, "Invalid temperature"
    assert config.max_tokens > 0, "Invalid max_tokens"
    
    print(f"       Temperature: {config.temperature}")
    print(f"       Max tokens: {config.max_tokens}")
    print(f"       Cache enabled: {config.enable_cache}")

check("LLM configuration loaded", check_llm_config)

def check_prompt_templates():
    """Check prompt templates"""
    from src.llm_prompt_templates import PROMPT_TEMPLATES, PromptTemplate
    
    required_templates = [
        PromptTemplate.VARIANT_EXPLANATION,
        PromptTemplate.RISK_EXPLANATION,
        PromptTemplate.DOSING_ADJUSTMENT,
    ]
    
    for template in required_templates:
        if template.value not in PROMPT_TEMPLATES:
            raise Exception(f"Missing template: {template.value}")
    
    print(f"       Templates loaded: {len(PROMPT_TEMPLATES)}")

check("Prompt templates configured", check_prompt_templates)

# ============================================================================
# 10. INTEGRATION TESTS
# ============================================================================
print("\n[10/10] INTEGRATION TESTS")

def check_complete_workflow():
    """Test complete workflow"""
    from src.risk_predictor import RiskPredictor
    import json
    from src.gene_models import Phenotype
    
    predictor = RiskPredictor()
    
    # Test 1: Simple genotype/phenotype
    genotypes = {"CYP2D6": ("*1", "*1")}
    phenotypes = {"CYP2D6": Phenotype.ULTRA_RAPID}
    drug_risks = {
        "Codeine": {
            "drug": "Codeine",
            "risk_level": "Toxic",
            "explanation": "Test",
            "dosing_recommendation": "Avoid",
            "monitoring": "Monitor",
            "cpic_level": "1A",
            "strength": "Strong",
            "clinical_guidance": "Test",
            "reference": "Test"
        }
    }
    
    output = predictor._generate_json_output(genotypes, phenotypes, drug_risks)
    data = json.loads(output)
    
    assert len(data) > 0, "No output"
    assert 'llm_generated_explanation' in data[0], "No LLM explanations"
    
    print(f"       JSON output: Valid")
    print(f"       LLM explanations: Generated")
    print(f"       Quality metrics: Included")

check("Complete workflow integration test", check_complete_workflow)

# ============================================================================
# SUMMARY
# ============================================================================
print("\n" + "="*90)
print("VERIFICATION SUMMARY")
print("="*90)

passed = len(results["passed"])
failed = len(results["failed"])
warnings_count = len(results["warnings"])

print(f"\n‚úÖ PASSED: {passed}/10")
for check_name in results["passed"]:
    print(f"   ‚úì {check_name}")

if failed > 0:
    print(f"\n‚ùå FAILED: {failed}")
    for check_name, error in results["failed"]:
        print(f"   ‚úó {check_name}")
        print(f"     {error[:80]}")

if warnings_count > 0:
    print(f"\n‚ö†Ô∏è  WARNINGS: {warnings_count}")
    for check_name, message in results["warnings"]:
        print(f"   ! {check_name}: {message}")

# Final verdict
print("\n" + "="*90)
if failed == 0:
    print("‚úÖ ALL SYSTEMS OPERATIONAL - READY FOR DEPLOYMENT")
    print("\nSystem Status:")
    print("  ‚úì LLM Integration: Active (Groq API)")
    print("  ‚úì API Endpoints: All 4 operational")
    print("  ‚úì Database: Functional (Cache + Job Manager)")
    print("  ‚úì VCF Processing: Working")
    print("  ‚úì LLM Explanations: Generating")
    print("  ‚úì Quality Metrics: Tracking")
    print("\nNext Steps:")
    print("  1. python api.py              (start REST API server)")
    print("  2. python test_api.py         (run API tests)")
    print("  3. streamlit run app.py       (start web app)")
    print("="*90)
    sys.exit(0)
else:
    print(f"‚ùå {failed} SYSTEM(S) NEED ATTENTION")
    print("="*90)
    sys.exit(1)
